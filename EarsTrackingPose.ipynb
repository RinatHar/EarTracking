{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGHRHH5g0gi1",
        "outputId": "e1fb6822-b868-48a8-81f7-fd156050b3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics opencv-python-headless supervision -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTPjmz5A0EIB",
        "outputId": "56ff1ee7-4a3d-4c22-eab8-13cf1501459b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import supervision as sv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uqCorLRB4GQl"
      },
      "outputs": [],
      "source": [
        "dataset = \"./EarsDetectionPoseBest/data.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzhU9Tf64CFh",
        "outputId": "38d3faaa-02c8-4b76-9a7d-8389d78eb996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt to 'yolo11n-pose.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.0MB 94.0MB/s 0.1s\n",
            "Ultralytics 8.3.241 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/ĞœĞ°Ğ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚ÑƒÑ€Ğ°/ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°/EarsDetectionPoseBest/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-pose.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/pose/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=pose, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 23.6MB/s 0.0s\n",
            "Overriding model.yaml kpt_shape=[17, 3] with kpt_shape=[1, 3]\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    502636  ultralytics.nn.modules.head.Pose             [1, [1, 3], [64, 128, 256]]   \n",
            "YOLO11n-pose summary: 196 layers, 2,661,804 parameters, 2,661,788 gradients, 6.7 GFLOPs\n",
            "\n",
            "Transferred 505/541 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 102.0MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.5Â±0.2 ms, read: 0.1Â±0.0 MB/s, size: 25.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/ĞœĞ°Ğ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚ÑƒÑ€Ğ°/ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°/EarsDetectionPoseBest/train/labels.cache... 1381 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1381/1381 2.0Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.7Â±0.1 ms, read: 0.1Â±0.0 MB/s, size: 24.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/ĞœĞ°Ğ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚ÑƒÑ€Ğ°/ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°/EarsDetectionPoseBest/valid/labels.cache... 49 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 49/49 60.6Kit/s 0.0s\n",
            "Plotting labels to /content/runs/pose/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 87 weight(decay=0.0), 97 weight(decay=0.0005), 96 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/pose/train\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/100      2.42G        1.4     0.1953     0.4122      1.675      1.225         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 1.1it/s 1:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.6s/it 7.2s\n",
            "                   all         49         55      0.947      0.927      0.963      0.615      0.947      0.927      0.966      0.955\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/100      2.57G      1.236    0.06109     0.1304     0.8261      1.087         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 29.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.0it/s 0.7s\n",
            "                   all         49         55       0.89      0.883      0.958      0.672      0.912      0.891      0.963      0.963\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      3/100      2.59G      1.193     0.0461    0.09954     0.8199      1.074          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 29.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.3it/s 0.9s\n",
            "                   all         49         55      0.921      0.836      0.934       0.61      0.921      0.836      0.936      0.936\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      4/100       2.6G      1.174    0.03728    0.08561     0.7748      1.064          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 28.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.6it/s 0.5s\n",
            "                   all         49         55      0.883      0.909       0.94      0.634      0.913      0.927      0.963      0.963\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      5/100       2.6G      1.136    0.03278    0.07816     0.7676      1.058          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 29.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.7s\n",
            "                   all         49         55       0.94      0.891      0.964      0.658       0.94      0.891       0.97       0.97\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      6/100      2.61G      1.127    0.03003      0.071     0.7154      1.043         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.6it/s 0.8s\n",
            "                   all         49         55      0.925      0.855      0.948      0.651      0.922      0.862      0.963      0.959\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      7/100      2.61G      1.118     0.0289    0.06602     0.7179      1.045         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 28.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.6it/s 0.8s\n",
            "                   all         49         55      0.907      0.909      0.967       0.66      0.907      0.909      0.971       0.97\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      8/100      2.61G      1.082    0.02618    0.06505     0.6754       1.03          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.0it/s 1.0s\n",
            "                   all         49         55      0.926      0.964      0.973       0.67      0.926      0.964      0.974      0.974\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      9/100      2.61G      1.072    0.02315    0.05965     0.6482      1.015          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.8it/s 1.1s\n",
            "                   all         49         55      0.964      0.945       0.98      0.695      0.964      0.945      0.984      0.984\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     10/100      2.61G      1.053    0.02206    0.05854     0.6424       1.01         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.9it/s 1.0s\n",
            "                   all         49         55      0.943      0.982      0.988       0.69      0.943      0.982      0.988      0.988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     11/100      2.61G       1.03    0.02269    0.06609     0.6333      1.006         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 29.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.6it/s 0.8s\n",
            "                   all         49         55      0.951      0.891      0.964      0.692      0.951      0.891      0.964      0.964\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     12/100      2.61G      1.013     0.0199     0.0569     0.6217     0.9959         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 29.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.7s\n",
            "                   all         49         55      0.929      0.945      0.982       0.69      0.929      0.945      0.982      0.982\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     13/100      2.62G      1.021    0.01791    0.06179     0.6112      1.003          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 29.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.4it/s 0.8s\n",
            "                   all         49         55       0.94      0.964      0.973      0.702       0.94      0.964      0.973      0.973\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     14/100      2.62G      1.002    0.01771    0.05342     0.6064     0.9944          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 2.9it/s 29.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s\n",
            "                   all         49         55      0.961      0.964      0.971      0.724      0.961      0.964      0.971      0.971\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     15/100      2.62G       0.97    0.01689    0.05105     0.5774     0.9794          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.6it/s 0.8s\n",
            "                   all         49         55      0.942      0.945      0.971      0.725      0.942      0.945      0.975      0.975\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     16/100      2.62G     0.9778      0.017    0.05561      0.584     0.9825          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s\n",
            "                   all         49         55      0.981      0.933      0.986      0.767      0.981      0.933      0.986      0.986\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     17/100      2.62G     0.9864    0.01824    0.06197     0.5748      0.982         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.7s\n",
            "                   all         49         55       0.98      0.964      0.983      0.708       0.98      0.964      0.984      0.984\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     18/100      2.62G     0.9476    0.01569    0.05308     0.5593     0.9787          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.2it/s 0.9s\n",
            "                   all         49         55      0.994      0.927       0.99      0.749      0.994      0.927       0.99       0.99\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     19/100      2.62G     0.9206     0.0148     0.0567     0.5379     0.9565          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s\n",
            "                   all         49         55          1      0.982      0.995      0.763          1      0.982      0.995      0.995\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     20/100      2.62G     0.9182    0.01453    0.05579     0.5455     0.9655          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.7s\n",
            "                   all         49         55      0.946      0.961      0.986      0.756      0.946      0.961      0.986      0.986\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     21/100      2.62G     0.9196    0.01533    0.05682     0.5355      0.962         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.8it/s 0.7s\n",
            "                   all         49         55      0.931      0.981      0.969      0.719      0.931      0.981      0.969      0.969\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     22/100      2.62G      0.918    0.01787    0.04841     0.5302      0.963          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.6it/s 0.8s\n",
            "                   all         49         55      0.982      0.979      0.985      0.747      0.982      0.979      0.985      0.985\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     23/100      2.62G     0.9062    0.01575    0.04581     0.5391     0.9542          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.8it/s 0.7s\n",
            "                   all         49         55          1      0.927      0.974      0.758          1      0.927      0.974      0.974\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     24/100      2.62G     0.8834    0.01513    0.06136     0.5293     0.9492         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.3it/s 0.9s\n",
            "                   all         49         55          1       0.94       0.97      0.745          1       0.94       0.97       0.97\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     25/100      2.62G     0.8925    0.01223    0.05619     0.5156      0.945         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.2it/s 0.9s\n",
            "                   all         49         55      0.974      0.964       0.98      0.756      0.974      0.964      0.985      0.985\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     26/100      2.62G     0.8608    0.01365    0.04907     0.5128     0.9339         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 28.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.3it/s 0.9s\n",
            "                   all         49         55       0.96      0.964      0.977      0.733       0.96      0.964      0.982      0.981\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     27/100      2.62G     0.8627    0.01347    0.04685     0.4961     0.9333         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.8it/s 1.1s\n",
            "                   all         49         55      0.963      0.943      0.982      0.764      0.963      0.943      0.982      0.982\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     28/100      2.62G     0.8571     0.0123    0.05168     0.5019     0.9298         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.0it/s 1.0s\n",
            "                   all         49         55       0.95      0.927      0.953      0.734       0.95      0.927      0.954      0.954\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     29/100      2.62G     0.8521    0.01333    0.05894     0.5006     0.9436         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.2it/s 0.9s\n",
            "                   all         49         55      0.981      0.957      0.984      0.763      0.981      0.957      0.984      0.984\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     30/100      2.62G      0.843    0.01303    0.04423     0.4964     0.9272         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s\n",
            "                   all         49         55      0.962      0.964      0.981      0.777      0.962      0.964      0.981      0.981\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     31/100      2.63G     0.8365    0.01122    0.03965     0.4852     0.9307         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55      0.928       0.94      0.968      0.741      0.928       0.94      0.973      0.972\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     32/100      2.63G     0.8246     0.0115    0.04908     0.4811     0.9206         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.7s\n",
            "                   all         49         55      0.947       0.98      0.977       0.74      0.947       0.98      0.982      0.981\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     33/100      2.63G     0.8218    0.01104    0.03567      0.476     0.9319         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55      0.976      0.964      0.981      0.768      0.976      0.964      0.982      0.982\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     34/100      2.63G     0.7918     0.0103    0.05588     0.4663     0.9141          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.8it/s 0.7s\n",
            "                   all         49         55      0.948      0.997      0.991        0.8      0.948      0.997      0.991      0.991\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     35/100      2.63G     0.7831    0.01061    0.03812     0.4594     0.9077          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55      0.944      0.945      0.968      0.752      0.944      0.945      0.968      0.968\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     36/100      2.63G     0.8005    0.01132    0.04179      0.463     0.9172          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.4it/s 0.8s\n",
            "                   all         49         55       0.98      0.964      0.987      0.783       0.98      0.964      0.987      0.987\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     37/100      2.63G     0.7714    0.01139    0.04548     0.4495     0.9067         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.0it/s 0.7s\n",
            "                   all         49         55      0.997      0.945      0.987      0.779      0.997      0.945      0.987      0.987\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     38/100      2.63G     0.7558    0.00984    0.03952     0.4357     0.9056          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55      0.977      0.982      0.987      0.737      0.977      0.982      0.993      0.991\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     39/100      2.63G     0.7777    0.01008    0.03628     0.4406     0.9117          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s\n",
            "                   all         49         55      0.978      0.945      0.982      0.773      0.978      0.945      0.982      0.982\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     40/100      2.63G      0.782    0.01172    0.04421     0.4548     0.9139          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55      0.972      0.964      0.988      0.783      0.972      0.964      0.988      0.988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     41/100      2.63G     0.7725     0.0113     0.0385     0.4493     0.9022         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.2it/s 0.9s\n",
            "                   all         49         55      0.997      0.964      0.994      0.785      0.997      0.964      0.994      0.994\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     42/100      2.63G      0.746    0.01079    0.04984     0.4341     0.8999         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.2it/s 0.9s\n",
            "                   all         49         55          1       0.98      0.992       0.78          1       0.98      0.992      0.992\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     43/100      2.63G     0.7334   0.009646    0.04144     0.4366     0.8956          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55      0.961      0.964      0.982      0.764      0.961      0.964      0.982      0.982\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     44/100      2.63G     0.7445   0.009902    0.05375     0.4384     0.9066          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.8it/s 0.5s\n",
            "                   all         49         55      0.998      0.964       0.99      0.782      0.998      0.964       0.99       0.99\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     45/100      2.63G     0.7395   0.009024    0.03591     0.4307     0.8964          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s\n",
            "                   all         49         55       0.98      0.964      0.991      0.792       0.98      0.964      0.991      0.991\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     46/100      2.63G     0.7333   0.009533     0.0401     0.4269     0.9004         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s\n",
            "                   all         49         55      0.964      0.982       0.99      0.785      0.964      0.982      0.991      0.991\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     47/100      2.63G     0.7267    0.00917    0.03943     0.4195     0.8959          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.7s\n",
            "                   all         49         55      0.998      0.982      0.989      0.777      0.998      0.982      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     48/100      2.63G     0.7095   0.009002    0.03865     0.4145      0.891          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.8s\n",
            "                   all         49         55      0.999      0.964      0.994      0.784      0.999      0.964      0.994      0.994\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     49/100      2.63G     0.7315   0.009558    0.04254      0.423     0.8961          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s\n",
            "                   all         49         55       0.96      0.982      0.986      0.804       0.96      0.982      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     50/100      2.63G     0.7274   0.009563    0.04261     0.4225     0.8994          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s\n",
            "                   all         49         55          1      0.961      0.987      0.789          1      0.961      0.987      0.987\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     51/100      2.63G       0.71   0.009307    0.03929     0.4144     0.8881          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.6it/s 0.6s\n",
            "                   all         49         55      0.981       0.94      0.989      0.785      0.981       0.94      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     52/100      2.63G     0.6931   0.008626    0.03996     0.4113     0.8838          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.7s\n",
            "                   all         49         55          1      0.942      0.992        0.8          1      0.942      0.992      0.992\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     53/100      2.63G     0.6732   0.009686    0.04195     0.3989     0.8763          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 28.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.7it/s 1.2s\n",
            "                   all         49         55          1      0.961      0.988      0.774          1      0.961      0.988      0.988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     54/100      2.64G     0.6777   0.008645    0.03679     0.3891     0.8779         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.1it/s 1.0s\n",
            "                   all         49         55      0.963      0.982      0.993      0.801      0.963      0.982      0.993      0.993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     55/100      2.64G      0.665   0.008845    0.04307     0.3943     0.8764          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.2it/s 0.9s\n",
            "                   all         49         55          1      0.964      0.985      0.801          1      0.964      0.985      0.985\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     56/100      2.64G     0.6733    0.00856    0.03862     0.3905     0.8805          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.4it/s 0.8s\n",
            "                   all         49         55      0.979      0.964      0.987      0.798      0.979      0.964      0.987      0.987\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     57/100      2.64G     0.6513   0.008083    0.03714     0.3842     0.8759          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 26.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.0it/s 0.7s\n",
            "                   all         49         55      0.981      0.962      0.986      0.785      0.981      0.962      0.986      0.986\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     58/100      2.64G      0.663   0.007829    0.03578     0.3859     0.8762          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s\n",
            "                   all         49         55          1       0.98      0.987      0.746          1       0.98      0.987      0.987\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     59/100      2.64G     0.6576   0.008656    0.03032     0.3917     0.8829          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.0it/s 0.7s\n",
            "                   all         49         55      0.998      0.964      0.993       0.81      0.998      0.964      0.993      0.993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     60/100      2.64G      0.668   0.008689    0.02861     0.3937     0.8777         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.8it/s 0.7s\n",
            "                   all         49         55      0.998      0.964       0.99      0.793      0.998      0.964       0.99       0.99\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     61/100      2.64G     0.6436   0.008764    0.03831     0.3751     0.8738          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55      0.998      0.964      0.993      0.789      0.998      0.964      0.993      0.993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     62/100      2.64G     0.6397    0.00854    0.04045     0.3708     0.8712         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s\n",
            "                   all         49         55      0.964       0.98      0.992      0.794      0.964       0.98      0.992      0.992\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     63/100      2.64G     0.6344   0.008184    0.03671     0.3758     0.8674         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.7s\n",
            "                   all         49         55          1      0.964      0.993      0.783          1      0.964      0.993      0.993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     64/100      2.64G     0.6319   0.008012     0.0317      0.374     0.8689         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 26.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s\n",
            "                   all         49         55      0.997      0.964      0.992      0.786      0.997      0.964      0.992      0.992\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     65/100      2.64G     0.6455   0.008388    0.04053     0.3838     0.8765          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55          1      0.963       0.99      0.799          1      0.963       0.99       0.99\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     66/100      2.64G     0.6117   0.008169     0.0336     0.3624      0.859         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.8it/s 0.7s\n",
            "                   all         49         55      0.997      0.964      0.992      0.785      0.997      0.964      0.992      0.992\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     67/100      2.64G     0.6099    0.00805    0.03776     0.3649     0.8687         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s\n",
            "                   all         49         55      0.982       0.98      0.988      0.773      0.982       0.98      0.988      0.988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     68/100      2.64G     0.6205   0.007953    0.03873      0.363     0.8668         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55      0.997      0.964       0.99      0.793      0.997      0.964       0.99       0.99\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     69/100      2.64G     0.5922   0.007967    0.03309     0.3591     0.8595         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.8it/s 0.7s\n",
            "                   all         49         55      0.999      0.982      0.992      0.801      0.999      0.982      0.992      0.992\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     70/100      2.64G      0.599   0.007147    0.03336     0.3547     0.8651         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s\n",
            "                   all         49         55          1      0.963      0.988      0.778          1      0.963      0.988      0.988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     71/100      2.64G     0.5842   0.008027    0.03892     0.3474     0.8646         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.0it/s 0.7s\n",
            "                   all         49         55      0.981      0.945      0.987      0.775      0.981      0.945      0.987      0.987\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     72/100      2.64G     0.5966   0.007493    0.03602     0.3518     0.8627          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.0it/s 0.7s\n",
            "                   all         49         55      0.982      0.979      0.989      0.777      0.982      0.979      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     73/100      2.64G      0.586   0.008235    0.03512     0.3501     0.8536         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.0it/s 0.7s\n",
            "                   all         49         55      0.999      0.945      0.991      0.798      0.999      0.945      0.991      0.991\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     74/100      2.64G     0.5818   0.007556    0.02986     0.3443     0.8574         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.9it/s 1.0s\n",
            "                   all         49         55      0.998      0.964      0.988      0.788      0.998      0.964      0.993      0.992\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     75/100      2.64G     0.5753   0.007447    0.03441     0.3415     0.8571          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.7it/s 1.2s\n",
            "                   all         49         55      0.999      0.964      0.991      0.798      0.999      0.964      0.992      0.991\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     76/100      2.64G      0.577   0.007209     0.0379     0.3442     0.8599          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.1it/s 27.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.0it/s 1.0s\n",
            "                   all         49         55      0.997      0.964      0.989      0.784      0.997      0.964      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     77/100      2.64G     0.5723   0.007525     0.0349     0.3367     0.8555          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.3it/s 0.9s\n",
            "                   all         49         55          1      0.963      0.993      0.783          1      0.963      0.993      0.993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     78/100      2.64G     0.5615   0.006804    0.02876     0.3399     0.8587         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.7s\n",
            "                   all         49         55      0.982       0.98      0.993      0.797      0.982       0.98      0.993      0.993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     79/100      2.64G     0.5486   0.007375    0.03931     0.3281     0.8452          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55          1      0.979       0.99      0.796          1      0.979       0.99       0.99\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     80/100      2.64G     0.5528    0.00728    0.03101     0.3342     0.8527         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.0it/s 0.7s\n",
            "                   all         49         55      0.991      0.964      0.989      0.789      0.991      0.964      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     81/100      2.64G     0.5498   0.007437    0.03021     0.3311     0.8546          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55      0.998      0.945      0.988      0.781      0.998      0.945      0.988      0.988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     82/100      2.64G     0.5384   0.006605    0.03482     0.3282     0.8499          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55          1      0.979      0.989      0.806          1      0.979      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     83/100      2.64G     0.5284   0.006669    0.02974     0.3223     0.8479         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.6it/s 0.8s\n",
            "                   all         49         55          1      0.981      0.989      0.793          1      0.981      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     84/100      2.64G     0.5367   0.008166    0.04369     0.3254     0.8497          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55      0.997      0.982      0.989      0.809      0.997      0.982      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     85/100      2.64G      0.527   0.007498    0.02896     0.3207     0.8469          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s\n",
            "                   all         49         55      0.979      0.982      0.989      0.786      0.979      0.982      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     86/100      2.64G     0.5221   0.006763    0.02546     0.3188     0.8441          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 26.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55      0.979      0.964      0.988      0.799      0.979      0.964      0.988      0.988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     87/100      2.64G     0.5124    0.00714    0.03222     0.3165     0.8472          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55      0.978      0.982      0.989      0.801      0.978      0.982      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     88/100      2.64G     0.5173   0.006815    0.03654     0.3127     0.8433         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55      0.994      0.982       0.99      0.811      0.994      0.982       0.99       0.99\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     89/100      2.64G     0.5204     0.0078    0.03134     0.3123     0.8444          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s\n",
            "                   all         49         55          1      0.962      0.989        0.8          1      0.962      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     90/100      2.64G     0.5082   0.008007    0.03638     0.3094     0.8425         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 26.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s\n",
            "                   all         49         55      0.982      0.973      0.989      0.789      0.982      0.973      0.989      0.989\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     91/100      2.65G      0.456   0.006248   0.004239     0.2647     0.8212          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.0it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55      0.998      0.982      0.988      0.782      0.998      0.982      0.988      0.988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     92/100      2.65G     0.4293   0.005804    0.00323     0.2496     0.8105          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.4it/s 25.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s\n",
            "                   all         49         55          1      0.981      0.989      0.781          1      0.981      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     93/100      2.65G     0.4286   0.005856   0.002303       0.25     0.8047          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.2it/s 27.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s\n",
            "                   all         49         55      0.997      0.982      0.989      0.788      0.997      0.982      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     94/100      2.65G      0.419   0.005848   0.002054     0.2448      0.808          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.7it/s 0.7s\n",
            "                   all         49         55      0.996      0.982      0.989      0.787      0.996      0.982      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     95/100      2.65G     0.4137   0.006096   0.002001     0.2458     0.8044          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.4it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55          1      0.977      0.989      0.779          1      0.977      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     96/100      2.65G     0.4057   0.006156   0.004383     0.2415     0.8006          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s\n",
            "                   all         49         55          1      0.962      0.989      0.784          1      0.962      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     97/100      2.65G     0.3994   0.006049   0.002431     0.2407       0.81          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.6it/s 0.8s\n",
            "                   all         49         55          1      0.963      0.989      0.789          1      0.963      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     98/100      2.65G      0.391   0.005559   0.001004     0.2355     0.8044          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.4it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.2it/s 0.6s\n",
            "                   all         49         55          1      0.962      0.989      0.785          1      0.962      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     99/100      2.65G     0.3871   0.005891   0.001813     0.2343     0.8017          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.1it/s 0.6s\n",
            "                   all         49         55          1      0.981      0.989      0.794          1      0.981      0.989      0.989\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    100/100      2.65G       0.39   0.005657   0.003175     0.2329     0.8043          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 87/87 3.3it/s 26.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n",
            "                   all         49         55          1      0.982      0.989      0.791          1      0.982      0.989      0.989\n",
            "\n",
            "100 epochs completed in 0.829 hours.\n",
            "Optimizer stripped from /content/runs/pose/train/weights/last.pt, 5.6MB\n",
            "Optimizer stripped from /content/runs/pose/train/weights/best.pt, 5.6MB\n",
            "\n",
            "Validating /content/runs/pose/train/weights/best.pt...\n",
            "Ultralytics 8.3.241 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n-pose summary (fused): 109 layers, 2,654,020 parameters, 0 gradients, 6.6 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n",
            "                   all         49         55      0.998      0.964      0.993      0.809      0.998      0.964      0.993      0.993\n",
            "Speed: 0.2ms preprocess, 3.8ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/pose/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 3. Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ YOLOv11n\n",
        "# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ YOLOv11n Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ½Ğ° COCO\n",
        "model = YOLO('yolo11n-pose.pt')\n",
        "\n",
        "# Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ\n",
        "results = model.train(data=dataset, epochs=100, imgsz=640)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pyNizGmqQMC-"
      },
      "outputs": [],
      "source": [
        "best_model = YOLO(\"best3.pt\")\n",
        "source = \"test/videos/6038241_Woman_Young_1280x720.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v26frcSiPJyx",
        "outputId": "fcf8daa4-804c-471a-855a-d0b89030d9b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 1 ear, 27.8ms\n",
            "Speed: 5.8ms preprocess, 27.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.2ms\n",
            "Speed: 0.9ms preprocess, 26.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.6ms\n",
            "Speed: 0.8ms preprocess, 28.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.7ms\n",
            "Speed: 0.9ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.7ms\n",
            "Speed: 1.1ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.8ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.7ms preprocess, 25.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.3ms\n",
            "Speed: 1.1ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.7ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.6ms\n",
            "Speed: 0.8ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.8ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.6ms\n",
            "Speed: 0.9ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.4ms\n",
            "Speed: 0.7ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.8ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.7ms preprocess, 24.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 0.8ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 1.0ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.7ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.0ms\n",
            "Speed: 0.9ms preprocess, 23.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.8ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.7ms preprocess, 24.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.7ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 30.6ms\n",
            "Speed: 0.7ms preprocess, 30.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.3ms\n",
            "Speed: 0.7ms preprocess, 22.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.7ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.6ms\n",
            "Speed: 0.8ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.6ms\n",
            "Speed: 0.7ms preprocess, 23.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.0ms\n",
            "Speed: 0.8ms preprocess, 25.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.4ms\n",
            "Speed: 0.9ms preprocess, 22.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.9ms\n",
            "Speed: 0.9ms preprocess, 23.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 1.0ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.8ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.8ms\n",
            "Speed: 0.7ms preprocess, 26.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.6ms\n",
            "Speed: 1.0ms preprocess, 25.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.9ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.6ms\n",
            "Speed: 0.9ms preprocess, 25.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.6ms\n",
            "Speed: 1.2ms preprocess, 28.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 0.9ms preprocess, 25.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 1.1ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.8ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 1.0ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.8ms preprocess, 25.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.8ms preprocess, 25.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 1.2ms preprocess, 25.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.5ms\n",
            "Speed: 0.8ms preprocess, 26.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.1ms\n",
            "Speed: 0.9ms preprocess, 28.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 26.3ms\n",
            "Speed: 1.0ms preprocess, 26.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 25.4ms\n",
            "Speed: 0.9ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 ears, 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.3ms\n",
            "Speed: 0.7ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.7ms\n",
            "Speed: 0.9ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.3ms\n",
            "Speed: 0.7ms preprocess, 23.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.7ms\n",
            "Speed: 0.9ms preprocess, 23.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.6ms\n",
            "Speed: 0.7ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.9ms\n",
            "Speed: 0.9ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 28.2ms\n",
            "Speed: 0.7ms preprocess, 28.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 34.3ms\n",
            "Speed: 1.0ms preprocess, 34.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.5ms\n",
            "Speed: 1.1ms preprocess, 28.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.3ms\n",
            "Speed: 1.0ms preprocess, 27.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.2ms\n",
            "Speed: 0.8ms preprocess, 27.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.3ms\n",
            "Speed: 0.9ms preprocess, 27.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.5ms\n",
            "Speed: 0.9ms preprocess, 27.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.9ms\n",
            "Speed: 0.9ms preprocess, 26.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 51.3ms\n",
            "Speed: 1.0ms preprocess, 51.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.5ms\n",
            "Speed: 0.9ms preprocess, 26.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 0.9ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.3ms\n",
            "Speed: 0.9ms preprocess, 26.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.0ms\n",
            "Speed: 0.9ms preprocess, 26.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.5ms\n",
            "Speed: 0.8ms preprocess, 24.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 1.0ms preprocess, 24.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.7ms preprocess, 25.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.6ms\n",
            "Speed: 0.7ms preprocess, 23.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.5ms\n",
            "Speed: 0.7ms preprocess, 23.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.3ms\n",
            "Speed: 0.9ms preprocess, 23.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.7ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.7ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 0.9ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.9ms\n",
            "Speed: 0.9ms preprocess, 25.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 0.8ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 27.0ms\n",
            "Speed: 1.0ms preprocess, 27.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.5ms\n",
            "Speed: 0.9ms preprocess, 28.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.0ms\n",
            "Speed: 1.0ms preprocess, 26.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 29.5ms\n",
            "Speed: 0.9ms preprocess, 29.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.2ms\n",
            "Speed: 1.2ms preprocess, 28.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.9ms\n",
            "Speed: 1.0ms preprocess, 26.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.5ms\n",
            "Speed: 0.9ms preprocess, 28.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.3ms\n",
            "Speed: 0.8ms preprocess, 26.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 25.8ms\n",
            "Speed: 0.9ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 25.4ms\n",
            "Speed: 0.9ms preprocess, 25.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 1.0ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.8ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 29.1ms\n",
            "Speed: 1.0ms preprocess, 29.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.5ms\n",
            "Speed: 0.7ms preprocess, 27.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.8ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 1.0ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 0.8ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.7ms preprocess, 24.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 49.1ms\n",
            "Speed: 0.9ms preprocess, 49.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.8ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 29.0ms\n",
            "Speed: 0.9ms preprocess, 29.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.0ms\n",
            "Speed: 0.9ms preprocess, 26.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.5ms\n",
            "Speed: 0.7ms preprocess, 25.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.1ms\n",
            "Speed: 0.9ms preprocess, 26.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 0.7ms preprocess, 24.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.8ms\n",
            "Speed: 0.9ms preprocess, 23.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 1.0ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 0.8ms preprocess, 24.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.9ms\n",
            "Speed: 0.8ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.9ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 1.0ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.7ms\n",
            "Speed: 1.0ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 1.0ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.1ms\n",
            "Speed: 0.9ms preprocess, 27.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.9ms\n",
            "Speed: 1.0ms preprocess, 26.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.5ms\n",
            "Speed: 1.0ms preprocess, 26.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.5ms\n",
            "Speed: 1.0ms preprocess, 28.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.6ms\n",
            "Speed: 1.0ms preprocess, 26.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.1ms\n",
            "Speed: 1.0ms preprocess, 26.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.9ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 1.0ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.8ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 0.9ms preprocess, 25.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 1.0ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.7ms preprocess, 25.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 1.0ms preprocess, 24.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 1.0ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.8ms preprocess, 25.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.8ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.6ms\n",
            "Speed: 0.9ms preprocess, 25.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.8ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.7ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 1.0ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 1.0ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.7ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.7ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 1.0ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 0.9ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.8ms preprocess, 24.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 0.9ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 1.1ms preprocess, 24.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.7ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 1.0ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.1ms\n",
            "Speed: 0.9ms preprocess, 28.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 35.2ms\n",
            "Speed: 0.7ms preprocess, 35.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.2ms\n",
            "Speed: 1.0ms preprocess, 27.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 1.0ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 0.7ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.6ms\n",
            "Speed: 0.9ms preprocess, 25.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 1.0ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 1.0ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.7ms preprocess, 24.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 30.0ms\n",
            "Speed: 1.0ms preprocess, 30.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 37.0ms\n",
            "Speed: 1.1ms preprocess, 37.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 41.0ms\n",
            "Speed: 1.0ms preprocess, 41.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 40.1ms\n",
            "Speed: 1.0ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.6ms\n",
            "Speed: 0.9ms preprocess, 25.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 0.8ms preprocess, 25.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.5ms\n",
            "Speed: 0.8ms preprocess, 25.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 1.0ms preprocess, 24.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.7ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 30.0ms\n",
            "Speed: 1.0ms preprocess, 30.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 43.5ms\n",
            "Speed: 1.3ms preprocess, 43.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 35.3ms\n",
            "Speed: 1.0ms preprocess, 35.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.1ms\n",
            "Speed: 1.1ms preprocess, 28.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.9ms\n",
            "Speed: 1.0ms preprocess, 28.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 42.0ms\n",
            "Speed: 1.1ms preprocess, 42.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 41.0ms\n",
            "Speed: 1.2ms preprocess, 41.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.1ms\n",
            "Speed: 0.8ms preprocess, 26.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.1ms\n",
            "Speed: 0.9ms preprocess, 27.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 1.0ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 1.0ms preprocess, 25.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.4ms\n",
            "Speed: 0.9ms preprocess, 26.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 1.0ms preprocess, 25.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 1.0ms preprocess, 25.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.6ms\n",
            "Speed: 0.9ms preprocess, 25.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 1.0ms preprocess, 25.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 0.9ms preprocess, 25.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.8ms\n",
            "Speed: 0.9ms preprocess, 25.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 0.7ms preprocess, 25.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 0.8ms preprocess, 25.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.0ms\n",
            "Speed: 0.9ms preprocess, 29.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 1.0ms preprocess, 25.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.9ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 0.8ms preprocess, 24.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.2ms\n",
            "Speed: 0.7ms preprocess, 24.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 1.0ms preprocess, 25.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 1.0ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.7ms preprocess, 25.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.7ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 1.0ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 1.1ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.6ms\n",
            "Speed: 1.0ms preprocess, 25.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.7ms\n",
            "Speed: 0.9ms preprocess, 25.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.0ms\n",
            "Speed: 1.1ms preprocess, 26.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 1.1ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.9ms\n",
            "Speed: 0.7ms preprocess, 25.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.5ms\n",
            "Speed: 0.9ms preprocess, 23.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 1.1ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.7ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 38.0ms\n",
            "Speed: 0.9ms preprocess, 38.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.8ms preprocess, 24.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.8ms\n",
            "Speed: 0.8ms preprocess, 23.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.7ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.8ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.0ms\n",
            "Speed: 0.8ms preprocess, 26.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.8ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.5ms\n",
            "Speed: 0.9ms preprocess, 26.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.9ms\n",
            "Speed: 1.0ms preprocess, 25.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.8ms\n",
            "Speed: 1.0ms preprocess, 26.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 1.0ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.7ms\n",
            "Speed: 1.0ms preprocess, 28.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.0ms\n",
            "Speed: 0.9ms preprocess, 27.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.7ms\n",
            "Speed: 1.0ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.8ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.8ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.6ms\n",
            "Speed: 0.8ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.8ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.8ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.8ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.6ms\n",
            "Speed: 0.7ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.7ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 1.1ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.8ms preprocess, 24.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 1.0ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.8ms preprocess, 25.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.6ms\n",
            "Speed: 0.9ms preprocess, 26.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.8ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.6ms\n",
            "Speed: 0.9ms preprocess, 25.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.7ms\n",
            "Speed: 0.9ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.8ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.7ms\n",
            "Speed: 1.0ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.8ms\n",
            "Speed: 1.0ms preprocess, 26.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.8ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.0ms\n",
            "Speed: 0.7ms preprocess, 26.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.8ms\n",
            "Speed: 0.8ms preprocess, 27.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.8ms\n",
            "Speed: 0.9ms preprocess, 23.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.7ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.8ms\n",
            "Speed: 0.8ms preprocess, 22.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.9ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.8ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.8ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.8ms\n",
            "Speed: 0.8ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.3ms\n",
            "Speed: 0.7ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.7ms\n",
            "Speed: 0.7ms preprocess, 26.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.8ms\n",
            "Speed: 0.9ms preprocess, 26.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.8ms preprocess, 24.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.9ms\n",
            "Speed: 0.7ms preprocess, 22.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.9ms\n",
            "Speed: 1.0ms preprocess, 22.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.0ms\n",
            "Speed: 0.8ms preprocess, 23.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 37.4ms\n",
            "Speed: 1.3ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.8ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.8ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.9ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 1.0ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 1.3ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.0ms\n",
            "Speed: 0.9ms preprocess, 27.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.8ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 1.0ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 1.0ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.6ms\n",
            "Speed: 1.0ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.7ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.9ms\n",
            "Speed: 0.9ms preprocess, 22.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.3ms\n",
            "Speed: 0.8ms preprocess, 23.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.1ms\n",
            "Speed: 1.0ms preprocess, 23.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.1ms\n",
            "Speed: 0.8ms preprocess, 23.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.8ms\n",
            "Speed: 0.8ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.3ms\n",
            "Speed: 0.9ms preprocess, 23.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.9ms\n",
            "Speed: 0.9ms preprocess, 25.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.8ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.8ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.7ms\n",
            "Speed: 0.8ms preprocess, 22.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 1.0ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.9ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 42.3ms\n",
            "Speed: 0.9ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 1.1ms preprocess, 24.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.5ms\n",
            "Speed: 0.9ms preprocess, 26.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 32.4ms\n",
            "Speed: 1.1ms preprocess, 32.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.8ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 1.0ms preprocess, 25.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.7ms\n",
            "Speed: 0.9ms preprocess, 25.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 1.0ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.9ms preprocess, 23.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.9ms\n",
            "Speed: 0.8ms preprocess, 22.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.8ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.5ms\n",
            "Speed: 0.8ms preprocess, 23.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.8ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.4ms\n",
            "Speed: 0.8ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 1.0ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.8ms\n",
            "Speed: 0.9ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.7ms\n",
            "Speed: 0.8ms preprocess, 22.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.9ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 0.8ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 21.9ms\n",
            "Speed: 0.7ms preprocess, 21.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.8ms\n",
            "Speed: 0.9ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.0ms\n",
            "Speed: 0.7ms preprocess, 23.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.9ms\n",
            "Speed: 0.8ms preprocess, 22.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.8ms\n",
            "Speed: 1.0ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.8ms\n",
            "Speed: 0.9ms preprocess, 27.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 0.9ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.5ms\n",
            "Speed: 0.9ms preprocess, 23.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 21.7ms\n",
            "Speed: 0.8ms preprocess, 21.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 51.3ms\n",
            "Speed: 1.0ms preprocess, 51.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.4ms\n",
            "Speed: 1.0ms preprocess, 27.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.9ms\n",
            "Speed: 0.9ms preprocess, 25.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.6ms\n",
            "Speed: 0.9ms preprocess, 25.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.5ms\n",
            "Speed: 0.9ms preprocess, 26.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.6ms\n",
            "Speed: 0.9ms preprocess, 25.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.8ms preprocess, 25.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 0.9ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.0ms\n",
            "Speed: 0.9ms preprocess, 26.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.7ms preprocess, 25.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.4ms\n",
            "Speed: 1.3ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 1.0ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 1.0ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.7ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.8ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 0.8ms preprocess, 23.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.7ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 1.0ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.9ms\n",
            "Speed: 0.7ms preprocess, 22.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.8ms\n",
            "Speed: 1.0ms preprocess, 26.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.7ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 1.0ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.8ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 1.0ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.8ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.8ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.7ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.8ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.8ms\n",
            "Speed: 0.9ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 47.2ms\n",
            "Speed: 0.8ms preprocess, 47.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 1.0ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.7ms\n",
            "Speed: 0.9ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.5ms\n",
            "Speed: 0.9ms preprocess, 26.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.8ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.0ms\n",
            "Speed: 0.8ms preprocess, 27.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 1.0ms preprocess, 24.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 1.0ms preprocess, 25.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.8ms preprocess, 24.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.8ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 33.3ms\n",
            "Speed: 1.4ms preprocess, 33.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 34.9ms\n",
            "Speed: 1.1ms preprocess, 34.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.1ms\n",
            "Speed: 1.1ms preprocess, 29.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.5ms\n",
            "Speed: 0.9ms preprocess, 25.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 1.0ms preprocess, 25.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.8ms\n",
            "Speed: 0.9ms preprocess, 25.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.3ms\n",
            "Speed: 0.9ms preprocess, 26.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.6ms\n",
            "Speed: 1.0ms preprocess, 28.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.2ms\n",
            "Speed: 1.0ms preprocess, 26.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.0ms\n",
            "Speed: 0.9ms preprocess, 26.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 1.0ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.8ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.9ms\n",
            "Speed: 0.8ms preprocess, 27.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 49.9ms\n",
            "Speed: 0.9ms preprocess, 49.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.2ms\n",
            "Speed: 0.9ms preprocess, 26.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.5ms\n",
            "Speed: 1.0ms preprocess, 28.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.3ms\n",
            "Speed: 0.9ms preprocess, 28.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.7ms\n",
            "Speed: 0.9ms preprocess, 28.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.1ms\n",
            "Speed: 1.0ms preprocess, 27.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.6ms\n",
            "Speed: 1.0ms preprocess, 27.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 1.0ms preprocess, 25.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.0ms\n",
            "Speed: 0.7ms preprocess, 24.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.2ms\n",
            "Speed: 0.9ms preprocess, 26.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.3ms\n",
            "Speed: 1.0ms preprocess, 26.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.0ms\n",
            "Speed: 1.0ms preprocess, 26.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.1ms\n",
            "Speed: 0.9ms preprocess, 27.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.6ms\n",
            "Speed: 1.0ms preprocess, 26.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.7ms\n",
            "Speed: 1.0ms preprocess, 26.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.3ms\n",
            "Speed: 0.9ms preprocess, 27.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.3ms\n",
            "Speed: 0.9ms preprocess, 27.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.6ms\n",
            "Speed: 1.0ms preprocess, 26.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.6ms\n",
            "Speed: 0.7ms preprocess, 25.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.0ms\n",
            "Speed: 0.8ms preprocess, 26.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.5ms\n",
            "Speed: 1.0ms preprocess, 24.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.9ms\n",
            "Speed: 0.9ms preprocess, 23.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.5ms\n",
            "Speed: 0.9ms preprocess, 26.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.9ms\n",
            "Speed: 0.8ms preprocess, 22.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.9ms\n",
            "Speed: 0.7ms preprocess, 23.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 0.7ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 0.7ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.4ms\n",
            "Speed: 0.9ms preprocess, 23.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 44.3ms\n",
            "Speed: 0.9ms preprocess, 44.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.0ms\n",
            "Speed: 0.9ms preprocess, 23.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.3ms\n",
            "Speed: 1.0ms preprocess, 24.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.8ms\n",
            "Speed: 0.9ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.6ms\n",
            "Speed: 0.8ms preprocess, 25.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 22.2ms\n",
            "Speed: 0.9ms preprocess, 22.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.8ms\n",
            "Speed: 0.9ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 ears, 22.5ms\n",
            "Speed: 0.8ms preprocess, 22.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.0ms\n",
            "Speed: 0.8ms preprocess, 23.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.9ms\n",
            "Speed: 0.8ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.4ms\n",
            "Speed: 0.8ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 0.7ms preprocess, 25.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.2ms\n",
            "Speed: 1.0ms preprocess, 24.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 1.0ms preprocess, 25.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 1.0ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 0.8ms preprocess, 24.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.6ms\n",
            "Speed: 0.9ms preprocess, 23.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 0.9ms preprocess, 25.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.0ms\n",
            "Speed: 0.8ms preprocess, 26.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 0.8ms preprocess, 25.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.8ms\n",
            "Speed: 1.0ms preprocess, 25.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.1ms\n",
            "Speed: 0.7ms preprocess, 24.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.7ms\n",
            "Speed: 1.0ms preprocess, 23.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.3ms\n",
            "Speed: 0.8ms preprocess, 24.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.0ms\n",
            "Speed: 0.9ms preprocess, 22.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.2ms\n",
            "Speed: 1.1ms preprocess, 24.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 37.6ms\n",
            "Speed: 1.2ms preprocess, 37.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.0ms\n",
            "Speed: 0.9ms preprocess, 22.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 1.0ms preprocess, 25.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 0.8ms preprocess, 24.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.1ms\n",
            "Speed: 0.7ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.0ms\n",
            "Speed: 0.9ms preprocess, 27.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.7ms\n",
            "Speed: 0.8ms preprocess, 22.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 1.0ms preprocess, 24.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 0.9ms preprocess, 24.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 1.0ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.3ms\n",
            "Speed: 0.8ms preprocess, 22.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.4ms\n",
            "Speed: 1.0ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.9ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 36.5ms\n",
            "Speed: 0.9ms preprocess, 36.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.1ms\n",
            "Speed: 1.0ms preprocess, 26.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.4ms\n",
            "Speed: 1.0ms preprocess, 26.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 26.5ms\n",
            "Speed: 0.9ms preprocess, 26.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.8ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 23.6ms\n",
            "Speed: 0.7ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.6ms\n",
            "Speed: 0.7ms preprocess, 22.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.1ms\n",
            "Speed: 0.9ms preprocess, 27.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 0.7ms preprocess, 24.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 21.8ms\n",
            "Speed: 1.0ms preprocess, 21.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 47.6ms\n",
            "Speed: 0.9ms preprocess, 47.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.2ms\n",
            "Speed: 1.0ms preprocess, 24.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.0ms\n",
            "Speed: 0.9ms preprocess, 24.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.7ms\n",
            "Speed: 0.8ms preprocess, 25.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.8ms\n",
            "Speed: 1.4ms preprocess, 31.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.6ms\n",
            "Speed: 1.0ms preprocess, 23.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.9ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.9ms\n",
            "Speed: 0.7ms preprocess, 22.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.4ms\n",
            "Speed: 0.7ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.4ms\n",
            "Speed: 1.0ms preprocess, 23.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.3ms\n",
            "Speed: 0.9ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.2ms\n",
            "Speed: 0.9ms preprocess, 22.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.8ms\n",
            "Speed: 1.0ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.4ms\n",
            "Speed: 0.9ms preprocess, 28.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.4ms\n",
            "Speed: 0.8ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.8ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 1.0ms preprocess, 25.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.8ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.6ms\n",
            "Speed: 1.0ms preprocess, 22.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.4ms\n",
            "Speed: 0.9ms preprocess, 23.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 1.0ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 0.9ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 37.8ms\n",
            "Speed: 3.0ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.0ms\n",
            "Speed: 0.7ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 0.7ms preprocess, 23.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.8ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.8ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.6ms\n",
            "Speed: 0.7ms preprocess, 22.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.5ms\n",
            "Speed: 0.7ms preprocess, 25.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.2ms\n",
            "Speed: 0.9ms preprocess, 26.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 1.2ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 0.8ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.9ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.7ms\n",
            "Speed: 0.7ms preprocess, 25.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.8ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 1.0ms preprocess, 25.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.9ms\n",
            "Speed: 1.0ms preprocess, 26.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.8ms\n",
            "Speed: 0.9ms preprocess, 27.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 27.1ms\n",
            "Speed: 1.0ms preprocess, 27.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.0ms\n",
            "Speed: 1.0ms preprocess, 26.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.3ms\n",
            "Speed: 1.0ms preprocess, 26.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 26.4ms\n",
            "Speed: 0.9ms preprocess, 26.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 ears, 26.2ms\n",
            "Speed: 0.9ms preprocess, 26.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.8ms\n",
            "Speed: 0.8ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.1ms\n",
            "Speed: 0.9ms preprocess, 23.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 31.0ms\n",
            "Speed: 0.9ms preprocess, 31.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.5ms\n",
            "Speed: 0.9ms preprocess, 23.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.8ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 38.9ms\n",
            "Speed: 0.8ms preprocess, 38.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.8ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.4ms\n",
            "Speed: 0.9ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.9ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.9ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.7ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.3ms\n",
            "Speed: 0.9ms preprocess, 25.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.6ms\n",
            "Speed: 0.7ms preprocess, 23.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.9ms\n",
            "Speed: 0.8ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.8ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.6ms\n",
            "Speed: 0.7ms preprocess, 23.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.7ms\n",
            "Speed: 0.9ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.1ms\n",
            "Speed: 0.9ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.3ms\n",
            "Speed: 0.9ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.2ms\n",
            "Speed: 1.0ms preprocess, 23.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.1ms\n",
            "Speed: 0.8ms preprocess, 24.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.7ms\n",
            "Speed: 0.7ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 23.9ms\n",
            "Speed: 0.7ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.2ms\n",
            "Speed: 0.9ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 28.3ms\n",
            "Speed: 0.9ms preprocess, 28.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.6ms\n",
            "Speed: 0.8ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.7ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.6ms\n",
            "Speed: 0.8ms preprocess, 25.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 22.6ms\n",
            "Speed: 0.8ms preprocess, 22.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.2ms\n",
            "Speed: 0.7ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 25.0ms\n",
            "Speed: 0.9ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 24.5ms\n",
            "Speed: 0.9ms preprocess, 24.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ear, 50.0ms\n",
            "Speed: 0.9ms preprocess, 50.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# ĞÑ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾\n",
        "cap = cv2.VideoCapture(source)\n",
        "\n",
        "# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ VideoWriter Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ\n",
        "output_path = \"processed_video.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # ĞºĞ¾Ğ´ĞµĞº Ğ´Ğ»Ñ MP4\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "# ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ´Ñ€Ñ‹\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Ğ”ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ\n",
        "    results = best_model(frame)\n",
        "\n",
        "    # Ğ Ğ¸ÑÑƒĞµĞ¼ Ğ±Ğ¾ĞºÑÑ‹ Ğ½Ğ° ĞºĞ°Ğ´Ñ€Ğµ\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "    # Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ´Ñ€\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "# ĞÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµĞ¼ Ñ€ĞµÑÑƒÑ€ÑÑ‹\n",
        "cap.release()\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vPO1NE8ch3k",
        "outputId": "b4e5b9b1-6a66-4e11-e407-5cdc2d9d07ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞµÑ€ĞµĞ¶ĞºĞ¸: 920x730 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹\n",
            "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞµÑ€ĞµĞ¶ĞºĞ¸: 500x396 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹\n",
            "\n",
            "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞµÑ€ĞµĞ¶ĞºĞ¸: 500x396 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹\n",
            "ĞĞ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾...\n",
            "ĞšĞ°Ğ´Ñ€ 30: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 60: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 90: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 120: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 150: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 180: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 210: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 240: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 270: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 300: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 330: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 360: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 390: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 420: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 450: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 480: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 510: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 540: ÑƒÑ…Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.20\n",
            "ĞšĞ°Ğ´Ñ€ 570: ÑƒÑ…Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.20\n",
            "ĞšĞ°Ğ´Ñ€ 600: ÑƒÑ…Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.20\n",
            "ĞšĞ°Ğ´Ñ€ 630: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 660: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 690: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "ĞšĞ°Ğ´Ñ€ 720: ÑƒÑ…Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: 0.30\n",
            "\n",
            "============================================================\n",
            "ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ!\n",
            "============================================================\n",
            "Ğ’ÑĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²: 726\n",
            "ĞšĞ°Ğ´Ñ€Ğ¾Ğ² Ñ ÑĞµÑ€ĞµĞ¶ĞºĞ¾Ğ¹: 556\n",
            "Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ: 76.6%\n",
            "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: /content/runs/pose/predict/6038241_Woman_Young_With_Earring.mp4\n",
            "ĞŸÑ€ĞµĞ²ÑŒÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: /content/preview_with_earring.jpg\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "earring_path = '/content/drive/MyDrive/ĞœĞ°Ğ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚ÑƒÑ€Ğ°/ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°/earring.png'\n",
        "earring_img = cv2.imread(earring_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "if earring_img is not None:\n",
        "    h, w = earring_img.shape[:2]\n",
        "    if max(h, w) > 500:\n",
        "        print(f\"Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞµÑ€ĞµĞ¶ĞºĞ¸: {w}x{h} Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹\")\n",
        "        scale_factor = 500.0 / max(h, w)\n",
        "        new_w = int(w * scale_factor)\n",
        "        new_h = int(h * scale_factor)\n",
        "        earring_img = cv2.resize(earring_img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "        print(f\"ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞµÑ€ĞµĞ¶ĞºĞ¸: {new_w}x{new_h} Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹\")\n",
        "else:\n",
        "    earring_img = np.zeros((100, 100, 4), dtype=np.uint8)\n",
        "    cv2.circle(earring_img, (50, 50), 40, (255, 0, 0, 255), -1)\n",
        "    cv2.circle(earring_img, (50, 50), 20, (0, 0, 255, 255), -1)\n",
        "\n",
        "# Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\n",
        "# best_model = YOLO('/content/drive/MyDrive/ĞœĞ°Ğ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚ÑƒÑ€Ğ°/ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°/best.pt')\n",
        "best_model = YOLO('/content/runs/pose/train/weights/best.pt')\n",
        "\n",
        "# ĞÑ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾\n",
        "source = \"/content/drive/MyDrive/ĞœĞ°Ğ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚ÑƒÑ€Ğ°/ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°/6038241_Woman_Young_1280x720.mp4\"\n",
        "cap = cv2.VideoCapture(source)\n",
        "\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "output_path = \"/content/runs/pose/predict/6038241_Woman_Young_With_Earring.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "def overlay_earring_safe(background, earring, x, y, scale=1.0):\n",
        "    if earring.shape[2] != 4:\n",
        "        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² RGBA\n",
        "        earring = cv2.cvtColor(earring, cv2.COLOR_BGR2BGRA)\n",
        "\n",
        "    h, w = earring.shape[:2]\n",
        "\n",
        "    if scale != 1.0:\n",
        "        new_w = max(1, int(w * scale))\n",
        "        new_h = max(1, int(h * scale))\n",
        "        earring_scaled = cv2.resize(earring, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "    else:\n",
        "        earring_scaled = earring\n",
        "\n",
        "    h_scaled, w_scaled = earring_scaled.shape[:2]\n",
        "\n",
        "    # Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ (Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒĞµĞ¼)\n",
        "    y1 = max(0, y - h_scaled//2)\n",
        "    y2 = min(background.shape[0], y + h_scaled//2)\n",
        "    x1 = max(0, x - w_scaled//2)\n",
        "    x2 = min(background.shape[1], x + w_scaled//2)\n",
        "\n",
        "    # Ğ•ÑĞ»Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼\n",
        "    if y1 >= y2 or x1 >= x2:\n",
        "        return background\n",
        "\n",
        "    # Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ\n",
        "    region_h = y2 - y1\n",
        "    region_w = x2 - x1\n",
        "\n",
        "    earring_y1 = max(0, h_scaled//2 - y)\n",
        "    earring_y2 = earring_y1 + region_h\n",
        "    earring_x1 = max(0, w_scaled//2 - x)\n",
        "    earring_x2 = earring_x1 + region_w\n",
        "\n",
        "    # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ ĞµÑĞ»Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞµÑ€ĞµĞ¶ĞºĞ¸\n",
        "    earring_y2 = min(h_scaled, earring_y2)\n",
        "    earring_x2 = min(w_scaled, earring_x2)\n",
        "\n",
        "    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ½ÑƒĞ¶Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞµÑ€ĞµĞ¶ĞºĞ¸\n",
        "    earring_part = earring_scaled[earring_y1:earring_y2, earring_x1:earring_x2]\n",
        "\n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹\n",
        "    if earring_part.shape[0] != region_h or earring_part.shape[1] != region_w:\n",
        "        # ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ\n",
        "        earring_part = cv2.resize(earring_part, (region_w, region_h))\n",
        "\n",
        "    if earring_part.shape[2] == 4:\n",
        "        alpha = earring_part[:, :, 3] / 255.0\n",
        "\n",
        "        for c in range(3):\n",
        "            background[y1:y2, x1:x2, c] = \\\n",
        "                alpha * earring_part[:, :, c] + \\\n",
        "                (1 - alpha) * background[y1:y2, x1:x2, c]\n",
        "    else:\n",
        "        background[y1:y2, x1:x2] = earring_part\n",
        "\n",
        "    return background\n",
        "\n",
        "# Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞµÑ€ĞµĞ¶ĞºĞ¸\n",
        "earring_h, earring_w = earring_img.shape[:2]\n",
        "print(f\"\\nĞ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞµÑ€ĞµĞ¶ĞºĞ¸: {earring_w}x{earring_h} Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹\")\n",
        "\n",
        "frame_count = 0\n",
        "processed_count = 0\n",
        "\n",
        "print(\"ĞĞ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾...\")\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    results = best_model(frame, verbose=False)\n",
        "\n",
        "    ear_detected = False\n",
        "    scale_used = 0.2\n",
        "\n",
        "    for result in results:\n",
        "        if result.keypoints is not None and len(result.keypoints.data) > 0:\n",
        "            kpt_data = result.keypoints.data[0].cpu().numpy()\n",
        "\n",
        "            if len(kpt_data) > 0 and kpt_data[0, 2] > 0.6:\n",
        "                x, y = int(kpt_data[0, 0]), int(kpt_data[0, 1])\n",
        "\n",
        "                if result.boxes is not None and len(result.boxes) > 0:\n",
        "                    bbox = result.boxes.data[0][:4].cpu().numpy()\n",
        "                    ear_width = bbox[2] - bbox[0]\n",
        "\n",
        "                    target_size = ear_width * 0.75\n",
        "\n",
        "                    # ĞœĞ°ÑÑˆÑ‚Ğ°Ğ± = Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ / Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞµÑ€ĞµĞ¶ĞºĞ¸\n",
        "                    scale_used = target_size / max(earring_w, earring_h)\n",
        "                    scale_used = max(0.3, min(1.0, scale_used))\n",
        "\n",
        "                    # Ğ”Ğ›Ğ¯ Ğ¡Ğ“Ğ›ĞĞ–Ğ˜Ğ’ĞĞĞ˜Ğ¯ Ğ”Ğ’Ğ˜Ğ–Ğ•ĞĞ˜Ğ¯\n",
        "                    if not hasattr(overlay_earring_safe, 'last_positions'):\n",
        "                        overlay_earring_safe.last_positions = []\n",
        "\n",
        "                    overlay_earring_safe.last_positions.append((x, y))\n",
        "                    if len(overlay_earring_safe.last_positions) > 5:\n",
        "                        overlay_earring_safe.last_positions.pop(0)\n",
        "\n",
        "                    # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… 5 Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹\n",
        "                    if len(overlay_earring_safe.last_positions) > 1:\n",
        "                        avg_x = int(np.mean([p[0] for p in overlay_earring_safe.last_positions]))\n",
        "                        avg_y = int(np.mean([p[1] for p in overlay_earring_safe.last_positions]))\n",
        "                        x, y = avg_x, avg_y\n",
        "\n",
        "                # ĞĞ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµĞ¼ ÑĞµÑ€ĞµĞ¶ĞºÑƒ\n",
        "                frame = overlay_earring_safe(frame, earring_img, x, y, scale_used)\n",
        "                ear_detected = True\n",
        "                processed_count += 1\n",
        "\n",
        "    # Ğ•ÑĞ»Ğ¸ ÑƒÑ…Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ\n",
        "    if not ear_detected and frame_count == 1:\n",
        "        print(\"Ğ£Ñ…Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ĞºĞ°Ğ´Ñ€Ğµ!\")\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "    if frame_count % 30 == 0:\n",
        "        print(f\"ĞšĞ°Ğ´Ñ€ {frame_count}: ÑƒÑ…Ğ¾ {'Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾' if ear_detected else 'Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾'}, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±: {scale_used:.2f}\")\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Ğ’ÑĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²: {frame_count}\")\n",
        "print(f\"ĞšĞ°Ğ´Ñ€Ğ¾Ğ² Ñ ÑĞµÑ€ĞµĞ¶ĞºĞ¾Ğ¹: {processed_count}\")\n",
        "print(f\"Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ: {processed_count/frame_count*100:.1f}%\")\n",
        "print(f\"Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: {output_path}\")\n",
        "\n",
        "cap = cv2.VideoCapture(output_path)\n",
        "ret, first_frame = cap.read()\n",
        "if ret:\n",
        "    results = best_model(first_frame, verbose=False)\n",
        "    for result in results:\n",
        "        if result.keypoints is not None:\n",
        "            kpt_data = result.keypoints.data[0].cpu().numpy()\n",
        "            if len(kpt_data) > 0:\n",
        "                x, y = int(kpt_data[0, 0]), int(kpt_data[0, 1])\n",
        "                cv2.circle(first_frame, (x, y), 5, (0, 0, 255), -1)\n",
        "                cv2.putText(first_frame, f\"Ear point\", (x+10, y),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
        "\n",
        "    preview = cv2.resize(first_frame, (width//2, height//2))\n",
        "    cv2.imwrite('/content/preview_with_earring.jpg', preview)\n",
        "    print(f\"ĞŸÑ€ĞµĞ²ÑŒÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: /content/preview_with_earring.jpg\")\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWnm4ePwADsC",
        "outputId": "e8dc0df8-baef-4557-a809-efb61f1b696e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ FPS\n",
            "\n",
            "==================================================\n",
            "Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« Ğ˜Ğ—ĞœĞ•Ğ Ğ•ĞĞ˜Ğ¯ FPS:\n",
            "==================================================\n",
            "Ğ’ÑĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾: 726\n",
            "ĞĞ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: 11.82 ÑĞµĞºÑƒĞ½Ğ´\n",
            "Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ FPS: 61.43\n",
            "Ğ’Ñ€ĞµĞ¼Ñ Ğ½Ğ° ĞºĞ°Ğ´Ñ€: 16.3 Ğ¼Ñ\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from ultralytics import YOLO\n",
        "\n",
        "best_model = YOLO('/content/runs/pose/train/weights/best.pt')\n",
        "source = \"/content/drive/MyDrive/ĞœĞ°Ğ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚ÑƒÑ€Ğ°/ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°/6038241_Woman_Young_1280x720.mp4\"\n",
        "\n",
        "print(\"Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ FPS\")\n",
        "\n",
        "# Ğ—Ğ°Ğ¼ĞµÑ€ÑĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾\n",
        "start_time_total = time.perf_counter()\n",
        "\n",
        "# Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ±ĞµĞ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµÑ€Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸\n",
        "results = best_model(source, stream=True, save=False, verbose=False)\n",
        "\n",
        "frame_count = 0\n",
        "for _ in results:\n",
        "    frame_count += 1\n",
        "\n",
        "end_time_total = time.perf_counter()\n",
        "total_time = end_time_total - start_time_total\n",
        "\n",
        "# Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ FPS\n",
        "if frame_count > 0 and total_time > 0:\n",
        "    fps = frame_count / total_time\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« Ğ˜Ğ—ĞœĞ•Ğ Ğ•ĞĞ˜Ğ¯ FPS:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Ğ’ÑĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾: {frame_count}\")\n",
        "    print(f\"ĞĞ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {total_time:.2f} ÑĞµĞºÑƒĞ½Ğ´\")\n",
        "    print(f\"Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ FPS: {fps:.2f}\")\n",
        "    print(f\"Ğ’Ñ€ĞµĞ¼Ñ Ğ½Ğ° ĞºĞ°Ğ´Ñ€: {1000/fps:.1f} Ğ¼Ñ\")\n",
        "    print(f\"{'='*50}\")\n",
        "else:\n",
        "    print(\"ĞÑˆĞ¸Ğ±ĞºĞ°: Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ¸Ğ»Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾ Ğ½ÑƒĞ»Ñ\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
